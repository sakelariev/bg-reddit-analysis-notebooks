{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the topic models I also made a small n-gram viewer app inspired by Google Books N-gram Viewer. This is the \n",
    "# script to prepare the n-gram combinations for that app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/bk8xzzt10l30g9c3zpfjjh700000gn/T/ipykernel_62102/173051064.py:4: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bulgarian = pd.read_csv(\"./data/bulgarian_comments.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.bg.stop_words import STOP_WORDS as BG_STOPWORDS\n",
    "from spacy.lang.en import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "bulgarian_df = pd.read_csv(\"./data/bulgarian_submissions.csv\")\n",
    "bulgarian = pd.read_csv(\"./data/bulgarian_comments.csv\")\n",
    "result_df = pd.concat([bulgarian, bulgarian_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Let's get only posts starting 2018, where bulgarian posts start to increase, otherwise due\n",
    "# to the low number of activity in first 10 years we would have flat line for almost all words until \n",
    "# 2018-2019\n",
    "result_df = result_df[result_df['year_month'] >= '2018-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## First quick and drity attempt at getting n-grams\n",
    "stopwords_bg = 'amp иначе xb gt та ама на от за да се по са ще че не това си като до през които най при но има след който към бъде той още може му което много със която или само тази те обаче във вече около както над така между ако лв им тези преди млн бе също пред ни когато защото кв би пък тъй ги ли пак според този все някои не'\n",
    "stopwords_custom = stopwords_bg.split()\n",
    "# stopwords_custom.append('не')\n",
    "\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = list(BG_STOPWORDS) + list(stop_words.STOP_WORDS) + stopwords_custom\n",
    "\n",
    "def prepare_ngram(text, ngrams):\n",
    "    # Take punctuations out\n",
    "    cleaned_string = re.sub('\\[,.*?“”…\\]', '', text)\n",
    "    cleaned_string = re.sub(r'[“”]', '', cleaned_string)\n",
    "\n",
    "    # Remove any digits\n",
    "    cleaned_string = ''.join([i for i in cleaned_string if not i.isdigit()])\n",
    "\n",
    "    # Tokenise the data\n",
    "    cleaned_string = re.sub('[%s]' % re.escape(string.punctuation), ' ', cleaned_string)\n",
    "\n",
    "    # Why did I needed to do lowercase? Double-check that - it doesn't match Google Ngram behavior\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    TOKENS = word_tokenize(cleaned_string) \n",
    "\n",
    "    # Filter those stop words out\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in TOKENS: \n",
    "        if w not in ADDITIONAL_STOPWORDS:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    # Count phrases\n",
    "    gram_df = pd.Series(nltk.ngrams(filtered_sentence, ngrams)).value_counts()\n",
    "    return gram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"bg_news_lg\")\n",
    "\n",
    "# Define your list of additional custom stop words\n",
    "stopwords_bg = 'amp the иначе xb gt та ама на от за да се по са ще че не това си като до през които най при но има след който към бъде той още може му което много със която или само тази те обаче във вече около както над така между ако лв им тези преди млн бе също пред ни когато защото кв би пък тъй ги ли пак според този все някои не'\n",
    "CUSTOM_STOPWORDS = stopwords_bg.split()\n",
    "\n",
    "# Add custom stop words to spaCy's default stop words set\n",
    "for word in CUSTOM_STOPWORDS:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def prepare_ngram_spacy(text, ngrams):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token.replace(\"-(се)\", \"\") for token in tokens]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = zip(*[tokens[i:] for i in range(ngrams)])\n",
    "    n_grams = [' '.join(n_gram) for n_gram in n_grams]\n",
    "\n",
    "    # Count phrases\n",
    "    ngram_counts = Counter(n_grams)\n",
    "    ngram_series = pd.Series(ngram_counts).sort_values(ascending=False)\n",
    "    return ngram_series\n",
    "\n",
    "\n",
    "def prepare_bigram_spacy(text, ngrams):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    tokens = [token.lower_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n_grams = zip(*[tokens[i:] for i in range(ngrams)])\n",
    "    n_grams = [' '.join(n_gram) for n_gram in n_grams]\n",
    "\n",
    "    # Count phrases\n",
    "    ngram_counts = Counter(n_grams)\n",
    "    ngram_series = pd.Series(ngram_counts).sort_values(ascending=False)\n",
    "    return ngram_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "реша полза               1\n",
       "agree to                 1\n",
       "мнение градя             1\n",
       "градя десетилетие        1\n",
       "десетилетие мой          1\n",
       "мой съзнателен           1\n",
       "съзнателен let           1\n",
       "let agree                1\n",
       "to disagree              1\n",
       "прокопиев кажа           1\n",
       "disagree айде            1\n",
       "айде весел               1\n",
       "весел посрещане          1\n",
       "посрещане успешен        1\n",
       "успешен здрав            1\n",
       "здрав местя              1\n",
       "кажа мнение              1\n",
       "аз прокопиев             1\n",
       "полза колебая            1\n",
       "гласувам надежда         1\n",
       "колебая секунда          1\n",
       "секунда мнение           1\n",
       "мнение полза             1\n",
       "полза отдалечаване       1\n",
       "отдалечаване русия       1\n",
       "русия гласувам           1\n",
       "надежда ппдб             1\n",
       "мнение аз                1\n",
       "ппдб стоя                1\n",
       "стоя американски         1\n",
       "американски посолство    1\n",
       "посолство идвам          1\n",
       "идвам различен           1\n",
       "различен мнение          1\n",
       "местя ес                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ако реша, че ми е от полза, няма да се колебая и за секунда. В момента съм на мнение, че най-много ми е от полза отдалечаване от Русия, затова гласувам с надеждата, че зад ппдб наистина стои американското посолство. Тук вече идват различните мнения - според теб е защото Прокопиев така ми е казал, според мен е мнение, градено с десетилетия в моя съзнателен живот. Let's agree to disagree. \\n\\nАйде весело посрещане и успешна и здрава година. Местя се в ЕС.\"\n",
    "prepare_ngram_spacy(text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset for unigrams\n",
    "full_df_unigram = result_df.groupby('created_utc').apply(lambda x: prepare_ngram_spacy(x['body'].str.cat(sep=', '), 1))\n",
    "full_df_unigram = full_df_unigram.reset_index().rename(columns={'created_utc' : 'date', 'level_1' : 'unigram', 0 : 'count'})\n",
    "full_df_unigram = full_df_unigram.explode('unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_bigram = result_df.groupby('created_utc').apply(lambda x: prepare_bigram_spacy(x['body'].str.cat(sep=', '), 2))\n",
    "full_df_bigram = full_df_bigram.reset_index().rename(columns={'created_utc' : 'date', 'level_1' : 'bigram', 0 : 'count'})\n",
    "full_df_bigram = full_df_bigram.explode('bigram')\n",
    "\n",
    "full_df_bigram = full_df_bigram[full_df_bigram['bigram'].str.len() > 2]\n",
    "# full_df_bigram = full_df_bigram.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare monthly counts and proportions for the combined dataframe (we want to be able to compare\n",
    "# unigrams to bigrams directly)\n",
    "full_df_unigram = full_df_unigram[full_df_unigram['unigram'].str.len() > 1]\n",
    "full_df_unigram.set_index('date', inplace=True)\n",
    "full_df_unigram.index = pd.DatetimeIndex(full_df_unigram.index)\n",
    "monthly_unigram = full_df_unigram.groupby([pd.Grouper(freq=\"M\"), \"unigram\"]).sum().reset_index()\n",
    "monthly_unigram['ratio'] = (monthly_unigram.groupby(['unigram','date'])['count'].transform(sum) / monthly_unigram.groupby('date')['count'].transform(sum))\n",
    "monthly_unigram = monthly_unigram.rename(columns={\"unigram\" : \"gram\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_bigram.set_index('date', inplace=True)\n",
    "full_df_bigram.index = pd.DatetimeIndex(full_df_bigram.index)\n",
    "monthly_bigram = full_df_bigram.groupby([pd.Grouper(freq=\"M\"), \"bigram\"]).sum().reset_index()\n",
    "monthly_bigram['ratio'] = (monthly_bigram.groupby(['bigram','date'])['count'].transform(sum) / monthly_bigram.groupby('date')['count'].transform(sum))\n",
    "monthly_bigram = monthly_bigram.rename(columns={\"bigram\" : \"gram\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_unigram.groupby(\"unigram\").sum().reset_index().sort_values(by=[\"count\"], ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter out everything that is only mentioned 4 times?\n",
    "grouped_bigram = full_df_bigram.groupby(\"bigram\").sum().reset_index().sort_values(by=[\"count\"], ascending=False)\n",
    "grouped_bigram = grouped_bigram[grouped_bigram['count'] > 9]\n",
    "keep_bigram = grouped_bigram['bigram'].values.tolist()\n",
    "monthly_bigram = monthly_bigram[monthly_bigram['gram'].isin(keep_bigram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's filter out everything that is only mentioned 4 times?\n",
    "grouped_unigram = full_df_unigram.groupby(\"unigram\").sum().reset_index().sort_values(by=[\"count\"], ascending=False)\n",
    "grouped_unigram = grouped_unigram[grouped_unigram['count'] > 7]\n",
    "keep_unigram = grouped_unigram['unigram'].values.tolist()\n",
    "monthly_unigram = monthly_unigram[monthly_unigram['gram'].isin(keep_unigram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_unigram.to_csv(\"./data/unigram_full_df.csv\")\n",
    "monthly_bigram.to_csv(\"./data/bigram_full_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analysis_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
